1.实验环境
所需要的包都在requirements.txt中,在根目录下运行pip3 install -e .就可以安装。不过torch 1.3.1我装的时候搜不到包，是从官网直接装的。
2.实验流程和命令
在ImageNet上的模型，先选取resnet18和resnet50进行训练
a.resnet18实验
compress_classifier.py文件在evaluate-project\examples目录下

pruning:
python3 compress_classifier.py -a=resnet18 --data ../../data.imagenet/ --workers 3 --batch-size 64 --epochs=100 --lr=0.001 --compress=../agp-pruning/resnet18.schedule_agp.yaml --pretrained --out-dir ../../outputsdata/



b.resnet50实验
pruning:
python3 compress_classifier.py -a=resnet50 --data ../../data.imagenet/ --workers 3 --batch-size 64 --epochs=100 --lr=0.005 --compress=../agp-pruning/resnet50.schedule_agp.yaml --pretrained --out-dir ../../outputsdata/ --gpu 0 --vs=0

knwoledge distilltion,compress to resne18:
python3 compress_classifier.py -a=resnet18  --lr 0.005 -p 50 --data ../../data.imagenet/ --epochs 100 --out-dir ../../outputsdata/ --wd=0.0002 --vs=0 --batch-size 64 --workers 3 --confusion --kd-teacher resnet50 --kd-pretrained --kd-temp 5.0 --kd-dw 0.7 --kd-sw 0.3 --no_quantization --reset-optimizer --gpu 0

knwoledge apgpruning+distilltion,compress to resne18:
python3 compress_classifier.py -a=resnet18  --lr 0.005 -p 50 --data ../../data.imagenet/ --epochs 100 --out-dir ../../outputsdata/ --wd=0.0002 --vs=0 --batch-size 64 --workers 3 --confusion --kd-teacher resnet50 --kd-resume  (path to pruned model) --kd-temp 5.0 --kd-dw 0.7 --kd-sw 0.3 --no_quantization --reset-optimizer --gpu 0

2/17 新增：
resnet50 knwoledge distilltion,compress to resne18 + pruning:
python3 compress_classifier.py -a=resnet18 --data ../../data.imagenet/ --workers 3 --batch-size 64 --epochs=100 --lr=0.001 --compress=../agp-pruning/resnet18.schedule_agp.yaml --pretrained --out-dir ../../outputsdata/ --resume-from  (path to knowledge distilltion model)

3/3 新增:
resnet18 knwoledge distilltion,compress to mobilenet:
python3 compress_classifier.py -a=mobilenet  --lr 0.005 -p 50 --data ../../data.imagenet/ --epochs 30 --out-dir ../../outputsdata/ --wd=0.0002 --vs=0 --batch-size 64 --workers 3 --confusion --kd-teacher resnet18 --kd-pretrained --kd-temp 5.0 --kd-dw 0.7 --kd-sw 0.3 --no_quantization --reset-optimizer --gpu 0

resnet18 kd+pruning:这个就是对上面跑出来的结果进行压缩
python3 compress_classifier.py -a=mobilenet --data ../../data.imagenet/ --workers 3 --batch-size 64 --epochs=50 --lr=0.005 --compress=../agp-pruning/mobilenet.imagenet.schedule_agp.yaml  --out-dir ../../outputsdata/ --gpu 0 --vs=0 --resume-from (path to resnet18,kd) --reset-optimizer

3/31新增：
resnet50 knwoledge distilltion,compress to resnet18:
python3 compress_classifier.py -a=resnet18  --lr 0.005 -p 50 --data ../../data.imagenet/ --epochs 30  --out-dir ../../outputsdata/ --wd=0.0002 --vs=0 --batch-size 64 --workers 3 --confusion --kd-teacher resnet50 --kd-pretrained --kd-temp 5.0 --kd-dw 0.7 --kd-sw 0.3 --no_quantization --reset-optimizer --gpu 0

4/31新增：
resnet50 knwoledge distilltion,compress to resnet34:
python3 compress_classifier.py -a=resnet34  --lr 0.005 -p 50 --data ../../data.imagenet/ --epochs 30  --out-dir ../../outputsdata/ --wd=0.0002 --vs=0 --batch-size 64 --workers 3  --kd-teacher resnet50 --kd-pretrained --kd-temp 5.0 --kd-dw 0.7 --kd-sw 0.3 --no_quantization --reset-optimizer --gpu 0

resnet34,pruning,对上面这个的结果压缩:
python3 compress_classifier.py -a=resnet34 --data ../../data.imagenet/ --workers 3 --batch-size 64 --epochs=50 --lr=0.005 --compress=../agp-pruning/resnet34.schedule_agp.yaml --resumf-from (path to resnet43) --out-dir ../../outputsdata/ --gpu 0 --vs=0




参数含义：
--data :path to dataset
--worker :number of data loading workers
--batch-size :mini-batch size
--epochs :number of total epochs to run
--out-dir :Path to dump logs and checkpoints
--compress :configuration file for pruning the model (default is to use hard-coded schedule)
--gpu  Comma-separated list of GPU device IDs to be used (default is to use all available devices)
--batch-size和--worker是我用自己电脑跑时候的设置，服务器上应该可以调大一点

distiller commands.txt里有完整的命令行参数解释






